{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgxy9yMAvgTN8cIjjHAng5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelMacLondon/ML/blob/main/K_Fold_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/"
      ],
      "metadata": {
        "id": "1MpC6RwmFKf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F47fmgu3EzBF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# correlation between test harness and ideal test condition\n",
        "from numpy import mean\n",
        "from numpy import isnan\n",
        "from numpy import asarray\n",
        "from numpy import polyfit\n",
        "from scipy.stats import pearsonr\n",
        "from matplotlib import pyplot\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create the dataset\n",
        "def get_dataset(n_samples=100):\n",
        "\tX, y = make_classification(n_samples=n_samples, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append(LogisticRegression())\n",
        "\tmodels.append(RidgeClassifier())\n",
        "\tmodels.append(SGDClassifier())\n",
        "\tmodels.append(PassiveAggressiveClassifier())\n",
        "\tmodels.append(KNeighborsClassifier())\n",
        "\tmodels.append(DecisionTreeClassifier())\n",
        "\tmodels.append(ExtraTreeClassifier())\n",
        "\tmodels.append(LinearSVC())\n",
        "\tmodels.append(SVC())\n",
        "\tmodels.append(GaussianNB())\n",
        "\tmodels.append(AdaBoostClassifier())\n",
        "\tmodels.append(BaggingClassifier())\n",
        "\tmodels.append(RandomForestClassifier())\n",
        "\tmodels.append(ExtraTreesClassifier())\n",
        "\tmodels.append(GaussianProcessClassifier())\n",
        "\tmodels.append(GradientBoostingClassifier())\n",
        "\tmodels.append(LinearDiscriminantAnalysis())\n",
        "\tmodels.append(QuadraticDiscriminantAnalysis())\n",
        "\treturn models\n",
        "\n",
        "# evaluate the model using a given test condition\n",
        "def evaluate_model(cv, model):\n",
        "\t# get the dataset\n",
        "\tX, y = get_dataset()\n",
        "\t# evaluate the model\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\t# return scores\n",
        "\treturn mean(scores)\n"
      ],
      "metadata": {
        "id": "GKhaC4PIE0T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define test conditions\n",
        "ideal_cv = LeaveOneOut()\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# get the list of models to consider\n",
        "models = get_models()\n",
        "# collect results\n",
        "ideal_results, cv_results = list(), list()\n",
        "# evaluate each model\n",
        "for model in models:\n",
        "\t# evaluate model using each test condition\n",
        "\tcv_mean = evaluate_model(cv, model)\n",
        "\tideal_mean = evaluate_model(ideal_cv, model)\n",
        "\t# check for invalid results\n",
        "\tif isnan(cv_mean) or isnan(ideal_mean):\n",
        "\t\tcontinue\n",
        "\t# store results\n",
        "\tcv_results.append(cv_mean)\n",
        "\tideal_results.append(ideal_mean)\n",
        "\t# summarize progress\n",
        "\tprint('>%s: ideal=%.3f, cv=%.3f' % (type(model).__name__, ideal_mean, cv_mean))\n",
        "\n"
      ],
      "metadata": {
        "id": "DnPIQi8bFD1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the correlation between each test condition\n",
        "corr, _ = pearsonr(cv_results, ideal_results)\n",
        "print('Correlation: %.3f' % corr)\n",
        "# scatter plot of results\n",
        "pyplot.scatter(cv_results, ideal_results)\n",
        "# plot the line of best fit\n",
        "coeff, bias = polyfit(cv_results, ideal_results, 1)\n",
        "line = coeff * asarray(cv_results) + bias\n",
        "pyplot.plot(cv_results, line, color='r')\n",
        "# label the plot\n",
        "pyplot.title('10-fold CV vs LOOCV Mean Accuracy')\n",
        "pyplot.xlabel('Mean Accuracy (10-fold CV)')\n",
        "pyplot.ylabel('Mean Accuracy (LOOCV)')\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "u8y9KiaaFH0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}